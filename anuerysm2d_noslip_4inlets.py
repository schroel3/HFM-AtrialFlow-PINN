# -*- coding: utf-8 -*-
"""Anuerysm2D_NoSlip_4inlets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14lOtVtRb_CBEcIbv81AAk3CH3C-TsY_T
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Utilities module

- neural_net class is used to create a fully connected neural network (customizable layers)
- input layers: spatial coordinates + time, output layers: pressure, concentration, velocity components
- Navier_Stokes functions solve PDEs (NS equations) - using tensorflow to solve numerically
- gradient functions calculate gradients of given functions wrt their input variables
"""

"""
@author: Maziar Raissi
"""

import tensorflow as tf
print(tf.__version__)
import numpy as np

from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import Session
from tensorflow.compat.v1 import global_variables_initializer


def tf_session():
    # tf session
    config = ConfigProto(allow_soft_placement=True,log_device_placement=True)
    config.gpu_options.force_gpu_compatible = True
    sess = Session(config=config)

    # init
    init = global_variables_initializer()
    sess.run(init)

    return sess

def relative_error(pred, exact):
    if type(pred) is np.ndarray:
        return np.sqrt(np.mean(np.square(pred - exact))/np.mean(np.square(exact - np.mean(exact))))
    return tf.sqrt(tf.reduce_mean(tf.square(pred - exact))/tf.reduce_mean(tf.square(exact - tf.reduce_mean(exact))))

def mean_squared_error(pred, exact):
    if type(pred) is np.ndarray:
        return np.mean(np.square(pred - exact))
    return tf.reduce_mean(tf.square(pred - exact))

def fwd_gradients(Y, x):
    dummy = tf.ones_like(Y)
    G = tf.gradients(Y, x, grad_ys=dummy, colocate_gradients_with_ops=True)[0]
    Y_x = tf.gradients(G, dummy, colocate_gradients_with_ops=True)[0]
    return Y_x

class neural_net(object):
    def __init__(self, *inputs, layers):

        self.layers = layers
        self.num_layers = len(self.layers)

        if len(inputs) == 0:
            in_dim = self.layers[0]
            self.X_mean = np.zeros([1, in_dim])
            self.X_std = np.ones([1, in_dim])
        else:
            X = np.concatenate(inputs, 1)
            self.X_mean = X.mean(0, keepdims=True)
            self.X_std = X.std(0, keepdims=True)

        self.weights = []
        self.biases = []
        self.gammas = []

        for l in range(0,self.num_layers-1):
            in_dim = self.layers[l]
            out_dim = self.layers[l+1]
            W = np.random.normal(size=[in_dim, out_dim])
            b = np.zeros([1, out_dim])
            g = np.ones([1, out_dim])
            # tensorflow variables
            self.weights.append(tf.Variable(W, dtype=tf.float32, trainable=True))
            self.biases.append(tf.Variable(b, dtype=tf.float32, trainable=True))
            self.gammas.append(tf.Variable(g, dtype=tf.float32, trainable=True))

    def __call__(self, *inputs):

        H = (tf.concat(inputs, 1) - self.X_mean)/self.X_std

        weights = self.weights
        for l in range(0, self.num_layers-1):
            W = self.weights[l]
            b = self.biases[l]
            g = self.gammas[l]
            # weight normalization
            V = W/tf.norm(W, axis = 0, keepdims=True)
            # matrix multiplication
            H = tf.matmul(H, V)
            # add bias
            H = g*H + b
            # activation
            if l < self.num_layers-2:
                H = H*tf.sigmoid(H)

        Y = tf.split(H, num_or_size_splits=H.shape[1], axis=1) # splits a tensor 'value' into a list of sub tensors

        return Y#, weights

def Navier_Stokes_2D(c, u, v, p, t, x, y, Pec, Rey):

    Y = tf.concat([c, u, v, p], 1)

    Y_t = fwd_gradients(Y, t)
    Y_x = fwd_gradients(Y, x)
    Y_y = fwd_gradients(Y, y)
    Y_xx = fwd_gradients(Y_x, x)
    Y_yy = fwd_gradients(Y_y, y)

    c = Y[:,0:1]
    u = Y[:,1:2]
    v = Y[:,2:3]
    p = Y[:,3:4]

    c_t = Y_t[:,0:1]
    u_t = Y_t[:,1:2]
    v_t = Y_t[:,2:3]

    c_x = Y_x[:,0:1]
    u_x = Y_x[:,1:2]
    v_x = Y_x[:,2:3]
    p_x = Y_x[:,3:4]

    c_y = Y_y[:,0:1]
    u_y = Y_y[:,1:2]
    v_y = Y_y[:,2:3]
    p_y = Y_y[:,3:4]

    c_xx = Y_xx[:,0:1]
    u_xx = Y_xx[:,1:2]
    v_xx = Y_xx[:,2:3]

    c_yy = Y_yy[:,0:1]
    u_yy = Y_yy[:,1:2]
    v_yy = Y_yy[:,2:3]

    e1 = c_t + (u*c_x + v*c_y) - (1.0/Pec)*(c_xx + c_yy)
    e2 = u_t + (u*u_x + v*u_y) + p_x - (1.0/Rey)*(u_xx + u_yy)
    e3 = v_t + (u*v_x + v*v_y) + p_y - (1.0/Rey)*(v_xx + v_yy)
    e4 = u_x + v_y

    #d = 1-c
    #d_t = 1-c_t
    #d_x = 1-c_x
    #d_y = 1-c_y
    #d_xx = 1-c_xx
    #d_yy = 1-c_yy
    #ed = d_t + (u*d_x + v*d_y) - (1.0/Pec)*(d_xx + d_yy)

    return e1, e2, e3, e4 #p #, ed

def Gradient_Velocity_2D(u, v, x, y):

    Y = tf.concat([u, v], 1)

    Y_x = fwd_gradients(Y, x)
    Y_y = fwd_gradients(Y, y)

    u_x = Y_x[:,0:1]
    v_x = Y_x[:,1:2]

    u_y = Y_y[:,0:1]
    v_y = Y_y[:,1:2]

    return [u_x, v_x, u_y, v_y]

def Strain_Rate_2D(u, v, x, y):

    [u_x, v_x, u_y, v_y] = Gradient_Velocity_2D(u, v, x, y)

    eps11dot = u_x
    eps12dot = 0.5*(v_x + u_y)
    eps22dot = v_y

    return [eps11dot, eps12dot, eps22dot]

def Navier_Stokes_3D(c, u, v, w, p, t, x, y, z, Pec, Rey):

    Y = tf.concat([c, u, v, w, p], 1)

    Y_t = fwd_gradients(Y, t)
    Y_x = fwd_gradients(Y, x)
    Y_y = fwd_gradients(Y, y)
    Y_z = fwd_gradients(Y, z)
    Y_xx = fwd_gradients(Y_x, x)
    Y_yy = fwd_gradients(Y_y, y)
    Y_zz = fwd_gradients(Y_z, z)

    c = Y[:,0:1]
    u = Y[:,1:2]
    v = Y[:,2:3]
    w = Y[:,3:4]
    p = Y[:,4:5]

    c_t = Y_t[:,0:1]
    u_t = Y_t[:,1:2]
    v_t = Y_t[:,2:3]
    w_t = Y_t[:,3:4]

    c_x = Y_x[:,0:1]
    u_x = Y_x[:,1:2]
    v_x = Y_x[:,2:3]
    w_x = Y_x[:,3:4]
    p_x = Y_x[:,4:5]

    c_y = Y_y[:,0:1]
    u_y = Y_y[:,1:2]
    v_y = Y_y[:,2:3]
    w_y = Y_y[:,3:4]
    p_y = Y_y[:,4:5]

    c_z = Y_z[:,0:1]
    u_z = Y_z[:,1:2]
    v_z = Y_z[:,2:3]
    w_z = Y_z[:,3:4]
    p_z = Y_z[:,4:5]

    c_xx = Y_xx[:,0:1]
    u_xx = Y_xx[:,1:2]
    v_xx = Y_xx[:,2:3]
    w_xx = Y_xx[:,3:4]

    c_yy = Y_yy[:,0:1]
    u_yy = Y_yy[:,1:2]
    v_yy = Y_yy[:,2:3]
    w_yy = Y_yy[:,3:4]

    c_zz = Y_zz[:,0:1]
    u_zz = Y_zz[:,1:2]
    v_zz = Y_zz[:,2:3]
    w_zz = Y_zz[:,3:4]

    e1 = c_t + (u*c_x + v*c_y + w*c_z) - (1.0/Pec)*(c_xx + c_yy + c_zz)
    e2 = u_t + (u*u_x + v*u_y + w*u_z) + p_x - (1.0/Rey)*(u_xx + u_yy + u_zz)
    e3 = v_t + (u*v_x + v*v_y + w*v_z) + p_y - (1.0/Rey)*(v_xx + v_yy + v_zz)
    e4 = w_t + (u*w_x + v*w_y + w*w_z) + p_z - (1.0/Rey)*(w_xx + w_yy + w_zz)
    e5 = u_x + v_y + w_z

    return e1, e2, e3, e4, e5

def Gradient_Velocity_3D(u, v, w, x, y, z):

    Y = tf.concat([u, v, w], 1)

    Y_x = fwd_gradients(Y, x)
    Y_y = fwd_gradients(Y, y)
    Y_z = fwd_gradients(Y, z)

    u_x = Y_x[:,0:1]
    v_x = Y_x[:,1:2]
    w_x = Y_x[:,2:3]

    u_y = Y_y[:,0:1]
    v_y = Y_y[:,1:2]
    w_y = Y_y[:,2:3]

    u_z = Y_z[:,0:1]
    v_z = Y_z[:,1:2]
    w_z = Y_z[:,2:3]

    return [u_x, v_x, w_x, u_y, v_y, w_y, u_z, v_z, w_z]

def Shear_Stress_3D(u, v, w, x, y, z, nx, ny, nz, Rey):

    [u_x, v_x, w_x, u_y, v_y, w_y, u_z, v_z, w_z] = Gradient_Velocity_3D(u, v, w, x, y, z)

    uu = u_x + u_x
    uv = u_y + v_x
    uw = u_z + w_x
    vv = v_y + v_y
    vw = v_z + w_y
    ww = w_z + w_z

    sx = (uu*nx + uv*ny + uw*nz)/Rey
    sy = (uv*nx + vv*ny + vw*nz)/Rey
    sz = (uw*nx + vw*ny + ww*nz)/Rey

    return sx, sy, sz

"""# Main code

- labeled data (for training): simulated data from CFD, unlabeled data: random points in the LA where PDEs need to be satisfied
- loss function has two components: data-driven (MSE - between NN predictions + reference data) + physics-informed (residual - between NS eqs at selected points in domain)


- training: NN parameters are adjusted to minimize the loss function - find parameters that fit both observed data and PDEs at specific points
"""

# Commented out IPython magic to ensure Python compatibility.
"""
@author: Maziar Raissi
"""

import tensorflow as tf
import numpy as np
import scipy.io
import time
import sys

import matplotlib.pyplot as plt
from matplotlib import pyplot

import tensorflow.compat.v1 as tf
tf.disable_eager_execution()
#tf.disable_v2_behavior()

# Set up Tensorflow to use GPU memory growth to allocate GPU memory as needed
# ConfigProto allocates operations to CPU/GPU + customize Tensorflow session
#from tensorflow.compat.v1 import ConfigProto
#from tensorflow.compat.v1 import InteractiveSession

#config = tf.ConfigProto()
#config.gpu_options.allow_growth = True

#session = tf.InteractiveSession(config=config)

#tf.test.is_gpu_available()
tf.config.list_physical_devices('GPU')


class HFM(object):
    # notational conventions:
    # _tf: placeholders for input/output data and points used to regress the equations
    # _pred: output of neural network
    # _eqns: points used to regress the equations
    # _data: input-output data
    # _star: predictions

    def __init__(self, t_data, x_data, y_data, c_data,
                       t_eqns, x_eqns, y_eqns,
                       t_inlet, x_inlet, y_inlet, u_inlet, v_inlet,
                       t_cyl, x_cyl, y_cyl,
                       layers, batch_size,
                       Pec, Rey):

        # specs
        self.layers = layers
        self.batch_size = batch_size

        # flow properties
        self.Pec = Pec
        self.Rey = Rey

        # data
        [self.t_data, self.x_data, self.y_data, self.c_data] = [t_data, x_data, y_data, c_data]
        [self.t_eqns, self.x_eqns, self.y_eqns] = [t_eqns, x_eqns, y_eqns] #do I add pressure and concentration here
        [self.t_inlet, self.x_inlet, self.y_inlet, self.u_inlet, self.v_inlet] = [t_inlet, x_inlet, y_inlet, u_inlet, v_inlet]
        [self.t_cyl, self.x_cyl, self.y_cyl] = [t_cyl, x_cyl, y_cyl]

        # placeholders
        [self.t_data_tf, self.x_data_tf, self.y_data_tf, self.c_data_tf] = [tf.placeholder(tf.float32, shape=[None, 1]) for _ in range(4)]
        [self.t_eqns_tf, self.x_eqns_tf, self.y_eqns_tf] = [tf.placeholder(tf.float32, shape=[None, 1]) for _ in range(3)]
        [self.t_inlet_tf, self.x_inlet_tf, self.y_inlet_tf, self.u_inlet_tf, self.v_inlet_tf] = [tf.placeholder(tf.float32, shape=[None, 1]) for _ in range(5)]
        [self.t_cyl_tf, self.x_cyl_tf, self.y_cyl_tf] = [tf.placeholder(tf.float32, shape=[None, 1]) for _ in range(3)]



        # physics "uninformed" neural networks
        self.net_cuvp = neural_net(self.t_data, self.x_data, self.y_data, layers = self.layers)

        [self.c_data_pred,
         self.u_data_pred,
         self.v_data_pred,
         self.p_data_pred] = self.net_cuvp(self.t_data_tf,
                                           self.x_data_tf,
                                           self.y_data_tf)

        # physics "uninformed" neural networks (data at the inlet)
        [_,
         self.u_inlet_pred,
         self.v_inlet_pred,
         _] = self.net_cuvp(self.t_inlet_tf,
                            self.x_inlet_tf,
                            self.y_inlet_tf)

        # physics "uninformed" neural networks (data on the cylinder)
        [_,
         self.u_cyl_pred,
         self.v_cyl_pred,
         _] = self.net_cuvp(self.t_cyl_tf,
                            self.x_cyl_tf,
                            self.y_cyl_tf)

        # Physics "informed" neural networks:
        # predict flow properties based on equations
        [self.c_eqns_pred,
         self.u_eqns_pred,
         self.v_eqns_pred,
         self.p_eqns_pred] = self.net_cuvp(self.t_eqns_tf,
                                           self.x_eqns_tf,
                                           self.y_eqns_tf)
        #self.ed_eqns_pred,
        [self.e1_eqns_pred,
         self.e2_eqns_pred,
         self.e3_eqns_pred,
         self.e4_eqns_pred] = Navier_Stokes_2D(self.c_eqns_pred,
                                    self.u_eqns_pred,
                                    self.v_eqns_pred,
                                    self.p_eqns_pred,
                                    self.t_eqns_tf,
                                    self.x_eqns_tf,
                                    self.y_eqns_tf,
                                    self.Pec,
                                    self.Rey)

        # Define the loss function:
        self.loss = mean_squared_error(self.c_data_pred, self.c_data_tf) + \
                    mean_squared_error(self.u_inlet_pred, self.u_inlet_tf) + \
                    mean_squared_error(self.v_inlet_pred, self.v_inlet_tf) + \
                    mean_squared_error(self.u_cyl_pred, 0.0) + \
                    mean_squared_error(self.v_cyl_pred, 0.0) + \
                    mean_squared_error(self.e1_eqns_pred, 0.0) + \
                    mean_squared_error(self.e2_eqns_pred, 0.0) + \
                    mean_squared_error(self.e3_eqns_pred, 0.0) + \
                    mean_squared_error(self.e4_eqns_pred, 0.0)
                   #mean_squared_error(self.p_outlet_pred, self.p_outlet_tf) + \


        # optimizers
        self.learning_rate = tf.placeholder(tf.float32, shape=[])
        self.optimizer = tf.train.AdamOptimizer(learning_rate = self.learning_rate)
        self.train_op = self.optimizer.minimize(self.loss)

        self.sess = tf_session()
       #self.saver = tf.train.Saver(keep_checkpoint_every_n_hours=1)
       #self.saver.save(self.sess,'/content/drive/MyDrive/Colab/BEng_Project/HFM/Results/Aneurysm2D/best_model/save_model')


    def train(self, total_time, learning_rate):

        N_data = self.t_data.shape[0]
        N_eqns = self.t_eqns.shape[0]

        start_time = time.time()
        running_time = 0
        it = 0
        while running_time < total_time:

            # select a random mini-batch
            idx_data = np.random.choice(N_data, self.batch_size) #min(self.batch_size, N_data)
            idx_eqns = np.random.choice(N_eqns, self.batch_size)

            (t_data_batch,
             x_data_batch,
             y_data_batch,
             c_data_batch) = (self.t_data[idx_data,:],
                              self.x_data[idx_data,:],
                              self.y_data[idx_data,:],
                              self.c_data[idx_data,:])

            (t_eqns_batch,
             x_eqns_batch,
             y_eqns_batch) = (self.t_eqns[idx_eqns,:],
                              self.x_eqns[idx_eqns,:],
                              self.y_eqns[idx_eqns,:])


            tf_dict = {self.t_data_tf: t_data_batch,
                       self.x_data_tf: x_data_batch,
                       self.y_data_tf: y_data_batch,
                       self.c_data_tf: c_data_batch,
                       #self.p_data_tf: p_data_batch,
                       self.t_eqns_tf: t_eqns_batch,
                       self.x_eqns_tf: x_eqns_batch,
                       self.y_eqns_tf: y_eqns_batch,
                       self.t_inlet_tf: self.t_inlet,
                       self.x_inlet_tf: self.x_inlet,
                       self.y_inlet_tf: self.y_inlet,
                       self.u_inlet_tf: self.u_inlet,
                       self.v_inlet_tf: self.v_inlet,
                       self.t_cyl_tf: self.t_cyl,
                       self.x_cyl_tf: self.x_cyl,
                       self.y_cyl_tf: self.y_cyl,
                       self.learning_rate: learning_rate}

            self.sess.run([self.train_op], tf_dict)
           #self.saver = tf.train.Saver(keep_checkpoint_every_n_hours=1)
           #self.saver.save(self.sess, 'best_model')

            # Print
            if it % 10 == 0:
                elapsed = time.time() - start_time
                running_time += elapsed/3600.0
                [loss_value,
                 learning_rate_value] = self.sess.run([self.loss,
                                                       self.learning_rate], tf_dict)
                print('It: %d, Loss: %.3e, Time: %.2fs, Running Time: %.2fh, Learning Rate: %.1e'
#                       %(it, loss_value, elapsed, running_time, learning_rate_value))
                sys.stdout.flush()
                start_time = time.time()
            it += 1

    def predict(self, t_star, x_star, y_star):

        tf_dict = {self.t_data_tf: t_star, self.x_data_tf: x_star, self.y_data_tf: y_star}

        c_star = self.sess.run(self.c_data_pred, tf_dict)
        u_star = self.sess.run(self.u_data_pred, tf_dict)
        v_star = self.sess.run(self.v_data_pred, tf_dict)
        p_star = self.sess.run(self.p_data_pred, tf_dict)

        return c_star, u_star, v_star, p_star



file_directory = '/content/drive/MyDrive/Colab/BEng_Project/HFM/Results/Aneurysm2D/'

if __name__ == "__main__":

    batch_size = 10000

    layers = [3] + 10*[4*50] + [4]

    # Load Data
    data = scipy.io.loadmat('/content/drive/MyDrive/Colab/BEng_Project/HFM/Data/split_LAA_no_slip_patient_4_1.mat')

    t_star = data['t_star']#[0:58,0:] # T x 1 = 50
    x_star = data['x_star']#[0:,0:58] # N x 1 = 66540
    y_star = data['y_star']#[0:,0:58] # N x 1

    T = t_star.shape[0]  # no. of time steps
    N = x_star.shape[0]  # no. of spatial points

    U_star = data['U_star']#[0:,0:58] # N x T = (66540,50)
    V_star = data['V_star']#[0:,0:58] # N x T
    P_star = data['P_star']#[0:,0:58] # N x T
    C_star = data['C_star']#[0:,0:58] # N x T
    patch_ID = data['patch_ID']


    # Rearrange Data
    T_star = np.tile(t_star, (1,N)).T # N x T = (66540,50)
    X_star = np.tile(x_star, (1,T)) # N x T
    Y_star = np.tile(y_star, (1,T)) # N x T

    # flatten arrays to one column (length NT)
    t = T_star.flatten()[:,None] # NT x 1
    x = X_star.flatten()[:,None] # NT x 1
    y = Y_star.flatten()[:,None] # NT x 1
    u = U_star.flatten()[:,None] # NT x 1
    v = V_star.flatten()[:,None] # NT x 1
    p = P_star.flatten()[:,None] # NT x 1
    c = C_star.flatten()[:,None] # NT x 1

    ######################################################################
    ######################## Training Data ###############################
    ######################################################################

    # Prepare training data:
    T_data = T # 50
    N_data = N # 66540

    idx_t = np.concatenate([np.array([0]), np.random.choice(T-2, T_data-2, replace=False)+1, np.array([T-1])] )
    idx_x = np.random.choice(N, N_data, replace=False)
    t_data = T_star[:, idx_t][idx_x,:].flatten()[:,None] #(3327000,1)
    x_data = X_star[:, idx_t][idx_x,:].flatten()[:,None] #(3327000,1)
    y_data = Y_star[:, idx_t][idx_x,:].flatten()[:,None] #(3327000,1)
    c_data = C_star[:, idx_t][idx_x,:].flatten()[:,None] #(3327000,1)
    #p_data = P_star[:, idx_t][idx_x,:].flatten()[:,None]

    T_eqns = T
    N_eqns = N
    idx_t = np.concatenate([np.array([0]), np.random.choice(T-2, T_eqns-2, replace=False)+1, np.array([T-1])] )
    idx_x = np.random.choice(N, N_eqns, replace=False)
    t_eqns = T_star[:, idx_t][idx_x,:].flatten()[:,None] #(3327000,1)
    x_eqns = X_star[:, idx_t][idx_x,:].flatten()[:,None] #(3327000,1)
    y_eqns = Y_star[:, idx_t][idx_x,:].flatten()[:,None] #(3327000,1)

    # Training Data on velocity (inlet)
    t_inlet = t[(np.where(patch_ID == 2)) or (np.where(patch_ID == 3)) or (np.where(patch_ID == 4)) or (np.where(patch_ID == 5))][:,None]
    x_inlet = x[(np.where(patch_ID == 2)) or (np.where(patch_ID == 3)) or (np.where(patch_ID == 4)) or (np.where(patch_ID == 5))][:,None]
    y_inlet = y[(np.where(patch_ID == 2)) or (np.where(patch_ID == 3)) or (np.where(patch_ID == 4)) or (np.where(patch_ID == 5))][:,None]
    u_inlet = u[(np.where(patch_ID == 2)) or (np.where(patch_ID == 3)) or (np.where(patch_ID == 4)) or (np.where(patch_ID == 5))][:,None]
    v_inlet = v[(np.where(patch_ID == 2)) or (np.where(patch_ID == 3)) or (np.where(patch_ID == 4)) or (np.where(patch_ID == 5))][:,None]

    # Training Data on velocity (cylinder)
    t_cyl = t[(np.where(patch_ID==1) or (np.where(patch_ID==7)))][:, None]
    x_cyl = x[(np.where(patch_ID==1)) or (np.where(patch_ID==7))][:, None]
    y_cyl = y[(np.where(patch_ID==1)) or (np.where(patch_ID==7))][:, None]


    # Training
    model = HFM(t_data, x_data, y_data, c_data,
                t_eqns, x_eqns, y_eqns,
                t_inlet, x_inlet, y_inlet, u_inlet, v_inlet,
                t_cyl, x_cyl, y_cyl,
                layers, batch_size,
                Pec = 7.5e6, Rey = 2000)

    model.train(total_time = 1, learning_rate=1e-3)

    # Test Data
    snap = np.array([48]) # 62 for test data

    data = scipy.io.loadmat('/content/drive/MyDrive/Colab/BEng_Project/HFM/Data/split_LAA_no_slip_patient_4_2.mat')

    t_star = data['t_star'] # T x 1 = 63
    x_star = data['x_star'] # N x 1 = 17909
    y_star = data['y_star'] # N x 1

    C_star = data['C_star'] # N x T = (17909,63)
    U_star = data['U_star'] # N x T
    V_star = data['V_star'] # N x T
    P_star = data['P_star'] # N x T

    # Prepare test data
    T = t_star.shape[0] # 63
    N = x_star.shape[0] # 17909

    # Rearrange Data
    T_star = np.tile(t_star, (1,N)).T # N x T = (17909,63)
    X_star = np.tile(x_star, (1,T)) # N x T
    Y_star = np.tile(y_star, (1,T)) # N x T

    t_test = T_star[:,snap] # N x T = (17909,63)
    x_test = X_star[:,snap]
    y_test = Y_star[:,snap]

    c_test = C_star[:,snap]
    u_test = U_star[:,snap]
    v_test = V_star[:,snap]
    p_test = P_star[:,snap]

    # Prediction
    c_pred, u_pred, v_pred, p_pred = model.predict(t_test, x_test, y_test)

    # Error
    error_c = relative_error(c_pred, c_test)
    error_u = relative_error(u_pred, u_test)
    error_v = relative_error(v_pred, v_test)
    #error_p = relative_error(p_pred, p_test)
    error_p = relative_error(p_pred - np.mean(p_pred), p_test - np.mean(p_test))

    print('Error c: %e' % (error_c))
    print('Error u: %e' % (error_u))
    print('Error v: %e' % (error_v))
    print('Error p: %e' % (error_p))

    ################# Save Data ###########################

    C_pred = 0*C_star #(17909,63)
    U_pred = 0*U_star
    V_pred = 0*V_star
    P_pred = 0*P_star

    C_error = 0*C_star #(17909,63)
    U_error = 0*U_star
    V_error = 0*V_star
    P_error = 0*P_star

    fig, ((ax1, ax2),(ax3,ax4)) = plt.subplots(2, 2)
    plt.figure(figsize=(25, 20))
    fig.suptitle('Errors for each Snapshot in Time')

    for snap in range(0,t_star.shape[0]): #0 to 62
        t_test = T_star[:,snap:snap+1] # (17909,0:1)... for each snap
        x_test = X_star[:,snap:snap+1]
        y_test = Y_star[:,snap:snap+1]

        c_test = C_star[:,snap:snap+1]
        u_test = U_star[:,snap:snap+1]
        v_test = V_star[:,snap:snap+1]
        p_test = P_star[:,snap:snap+1]

        # Prediction
        c_pred, u_pred, v_pred, p_pred = model.predict(t_test, x_test, y_test)

        C_pred[:,snap:snap+1] = c_pred
        U_pred[:,snap:snap+1] = u_pred
        V_pred[:,snap:snap+1] = v_pred
        P_pred[:,snap:snap+1] = p_pred

        # Error
        error_c = relative_error(c_pred, c_test)
        error_u = relative_error(u_pred, u_test)
        error_v = relative_error(v_pred, v_test)
        #error_p = relative_error(p_pred, p_test)
        error_p = relative_error(p_pred - np.mean(p_pred), p_test - np.mean(p_test))

        # Save error
        C_error[snap] = error_c #C_error[:,snap:snap+1]
        U_error[snap] = error_u
        V_error[snap] = error_v
        P_error[snap] = error_p

        ax1.scatter(snap, error_p, color='purple')
        ax2.scatter(snap, error_c, color='green')
        ax3.scatter(snap, error_u, color='red')
        ax4.scatter(snap, error_v, color='cyan')

        print('Error c: %e' % (error_c))
        print('Error u: %e' % (error_u))
        print('Error v: %e' % (error_v))
        print('Error p: %e' % (error_p))


    # Set titles and labels
    ax1.set_title('Pressure Field')
    ax1.set(xlabel='Snapshot in Time', ylabel='Error')

    ax2.set_title('Concentration Field')
    ax2.set(xlabel='Snapshot in Time', ylabel='Error')

    ax3.set_title('"U" Velocity Field')
    ax3.set(xlabel='Snapshot in Time', ylabel='Error')

    ax4.set_title('"V" Velocity Field')
    ax4.set(xlabel='Snapshot in Time', ylabel='Error')

    fig.show()
    fig.tight_layout()
    fig.savefig(file_directory + 'PINN_loss_%s' % (time.strftime('%d_%m_%Y')))
    pyplot.close()


    scipy.io.savemat('/content/drive/MyDrive/Colab/BEng_Project/HFM/Results/Aneurysm2D/Anuerysm2D_results_%s.mat' %(time.strftime('%d_%m_%Y')),
                     {'C_pred':C_pred, 'U_pred':U_pred, 'V_pred':V_pred,'P_pred':P_pred})

    scipy.io.savemat('/content/drive/MyDrive/Colab/BEng_Project/HFM/Results/Aneurysm2D/Anuerysm2D_errors_%s.mat' %(time.strftime('%d_%m_%Y')),
        {'C_error': C_error, 'U_error': U_error, 'V_error': V_error,'P_error': P_error})